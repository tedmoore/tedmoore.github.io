<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine-learning on Ted Moore | Composer, Improviser, Intermedia Artist</title><link>https://tedmoore.github.io/tags/machine-learning/</link><description>Recent content in machine-learning on Ted Moore | Composer, Improviser, Intermedia Artist</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Ted Moore. All Rights Reserved.</copyright><lastBuildDate>Thu, 25 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://tedmoore.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Learned Navigation of StyleGAN3 Latent Space from Audio Descriptors</title><link>https://tedmoore.github.io/blog/learned-navigation-stylegan3/</link><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/learned-navigation-stylegan3/</guid><description>In September 2025, I presented a paper at the 6th Artificial Intelligence and Music Creativity Conference, which was held in Brussels Belgium. My paper described a process creating an audio reactive video (for the fourth movement of arco) using some machine learning in an attempt to intimately link the audio and video morphologies.
You can read the full paper here: Learned Navigation of StyleGAN3 Latent Space from Audio Descriptors
Screenshot of SuperCollider program developed for creating the audio reactive video.</description></item><item><title>_alloy_ the album</title><link>https://tedmoore.github.io/blog/alloy-album/</link><pubDate>Fri, 09 May 2025 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/alloy-album/</guid><description>After having performed alloy many times with Kyle, we decided to record a whole album of just feedback cymbal + saxophone multiphonics.
Track 1 from alloy:
Your browser does not support the audio element. see the code referenced below.
Multiphonic Similarity Lookup The main technological idea behind the album was to use a system that would analyze the &amp;ldquo;pitch class content&amp;rdquo; of the feedback cymbal in real-time, find the saxophone multiphonic with the most similar pitch class content, and display it to the saxophonist to perform, (perhaps) blending harmonically with the cymbal (I say perhaps because the microtonality of the cymbal and multiphonics makes for some juicy proximities).</description></item><item><title>_Musical Agents, Agency, &amp; AI: Towards a Phenomenological Understanding_</title><link>https://tedmoore.github.io/blog/agency/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/agency/</guid><description>Here&amp;rsquo;s the paper I presented at ICMC 2024 titled, Musical Agents, Agency, &amp;amp; AI: Towards a Phenomenological Understanding.
Read Full Paper
abstract:
Creating AI musical agents is an interest to many electronic music artists and researchers. This paper suggests a phenomenological understanding of &amp;ldquo;musical agency&amp;rdquo; determined by four criteria: in order to perceive a technological system as a collaborative agent it must (1) be perceived as separate from the user, (2) be surprising to the user, (3) be intended as a collaborative agent, and (4) mirror the user’s own musical intentions.</description></item><item><title>Adapting _saccades_ for the Allosphere</title><link>https://tedmoore.github.io/blog/allosphere/</link><pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/allosphere/</guid><description>published May 15, 2025
In spring of 2024 I was asked to present saccades in the Allosphere: a three-story, 360 degree video sphere with 54 channel surround sound at UC Santa Barbara. The audience watches from a bridge that goes directly through the middle of the sphere. My saxophone-playing collaborator Kyle Hutchins was doing a west coast tour and though it would be cool to bring saccades to this very tech-forward space.</description></item><item><title>Creating *quartet*</title><link>https://tedmoore.github.io/blog/creating-quartet/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/creating-quartet/</guid><description>published May 15, 2025
I created quartet in collaboration with the [Switch Ensemble~] for the 2021 SEAMUS Conference which was held online because of the pandemic. Because I knew that the work&amp;rsquo;s &amp;ldquo;premiere&amp;rdquo; would be held as a prerecorded video concert on YouTube, I decided to approach the work not as a live, electro-acoustic composition, but as a fixed-media, video-art piece that made use of the of the necessary technological mediation at play.</description></item><item><title>The Harmonic Series Strikes Again</title><link>https://tedmoore.github.io/blog/harmonic-series-strikes-again/</link><pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/harmonic-series-strikes-again/</guid><description>published May 17, 2025
My composition hollow, includes large PVC tubes that are used in a complex audio feedback system. The three tubes (all four inches in diameter) are cut to lengths of 10 feet, 8.4 feet, and 7.5 feet in order to achieve resonances at 55.8 Hz (~A1), 66.3 Hz (~C2), and 74.4 Hz (~D2) respectively.
A recording of a performance of hollow was used as the tape part in the fifth movement of arco.</description></item><item><title>alloy</title><link>https://tedmoore.github.io/works/alloy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/alloy/</guid><description>alloy uses the same feedback cymbal setup as in my solo work it teaches us that it doesn&amp;rsquo;t exist, however this duo also includes an instrument that can produce multiphonics. During performance, the feedback cymbal&amp;rsquo;s audio is analyzed for chroma and in real-time compared to recordings of the instrumetalist&amp;rsquo;s multiphonics (that were recorded and analyzed earlier). The multiphonic most similar to the current cymbal timbre is displayed on a screen for the instrumentalist, who may choose to either play the displayed multiphonic or deviate from it, as the musical moment offers.</description></item><item><title>arco</title><link>https://tedmoore.github.io/works/arco/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/arco/</guid><description>arco was motivated by an suggestion from violinist Marco Fusi to create a solo violin version of my work triangle for string quartet and tape. I liked the idea but felt that it would make more sense alongside additional movements to provide some context and commentary on this solo version. arco is a five movement work of which the solo version of triangle is movement four. I also created video designs to accompany the violinist and tape parts.</description></item><item><title>feed</title><link>https://tedmoore.github.io/works/feed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/feed/</guid><description>feed was created in collaboration with bassoonist Ben Roidl-Ward for Experimental Sound Studio&amp;rsquo;s Oscillation Series.
feed integrates multiple modes of real-time lighting control using custom tools that implement DMX parameterization, machine learning, music information retrieval, and Reaper sequencer integration. My DMX parameter control system is a family of OOP classes written in SuperCollider that can be implemented with any DMX compatible lights. It allows for user-defined parameter naming and contains built-in timed fades, LFO modulation, control bus following (for audio reactivity), and user-defined function control of individual parameters.</description></item><item><title>FluCoMa</title><link>https://tedmoore.github.io/research/flucoma/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/research/flucoma/</guid><description>Registration open for the FluCoMa Workshop Summer 2024
During the 2021-2022 academic year I served as a postdoctoral Research Fellow in Creative Coding at the University of Huddersfield CeReNeM working on the project Fluid Corpus Manipulation (&amp;ldquo;FluCoMa&amp;rdquo;).
FluCoMa enables techno-fluent musicians to integrate machine listening and machine learning in their creative practice within Max, SuperCollider, and Pure Data. The toolkit offers tools to separate audio into component parts including slicers and spectral decomposition algorithms, audio analysis tools to describe audio components as analytical and statistical representations, data analysis and machine learning algorithms for pattern detection and expressive dataset browsing, and audio morphing and hybridization algorithms for audio remixing, interpolating, and variation-making.</description></item><item><title>hollow</title><link>https://tedmoore.github.io/works/hollow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/hollow/</guid><description>hollow was created in collaboration with saxophonist Kyle Hutchins and is featured on Binary Canary&amp;rsquo;s album iterative systems out on Carrier Records.
Three large PVC tubes (4 inches in diameter and between 7 and 10 feet long–they can be seen in the video) are amplified by placing a microphone on one end and a speaker on the other. The feedback this creates is stable only at the resonant frequencies of the tube.</description></item><item><title>Module Tensor</title><link>https://tedmoore.github.io/research/module-tensor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/research/module-tensor/</guid><description>Module Tensor is the custom software I coded in SuperCollider and use to improvise on live electronics. It is based on a system of routing audio through processing modules and maximal flexibility of control with any MIDI or OSC devices. The primary use is live laptop improvisation, but it is also used as a framework for executing performances of my compositions. The conceptual structuring of this software is based on the research of laptop improviser Sam Pluta.</description></item><item><title>nand</title><link>https://tedmoore.github.io/works/nand/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/nand/</guid><description>nand was premiered by the Splice Ensemble at their 2023 Splice Festival in Boston. nand is based on the timbre and rhythmic gating of the NAND-gate feedback circuit described by Nick Collins in Handmade Electronic Music. Even though the system is quite simple (producing repeating phrases consisting of square waves, filtered noise, and silence), each gesture has microvariations that increase the entropy and attract my attention endlessly. My favorite timbres from this circuit occur while parameters are being changed–when capacitors are firing at surprising times, before they can settle into a stasis.</description></item><item><title>noise/gate</title><link>https://tedmoore.github.io/works/noise-gate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/noise-gate/</guid><description> Composed for Giacomo Piermatti of Ensemble Suono Giallo for ilSuono Contemporary Music Week 2021. Città di Castello, Italy.
full score</description></item><item><title>quartet</title><link>https://tedmoore.github.io/works/quartet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/quartet/</guid><description>created with the [Switch~ Ensemble]:
Zach Sheets, flute
T.J. Borden, cello
Wei-Han Wu, piano
Megan Arns, percussion
quartet is a remote collaboration between myself and the [Switch~ Ensemble] designed to engage with the added technological mediation at play during the pandemic. The sonic source material of quartet is about two minutes of eurorack synthesizer recordings transcribed for the [Switch~ Ensemble] to record. These recordings were then subjected to data analysis using audio descriptors and machine learning algorithms using FluCoMa.</description></item><item><title>saccades</title><link>https://tedmoore.github.io/works/saccades/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/saccades/</guid><description>A “saccade” is a rapid movement of the eyeball between two fixed focal points. During this brief moment, the brain hides this blurry motion from our perception. Once a saccade motion has begun, the destination cannot change, meaning that if the target of focus disappears the viewer won’t know until the saccade completes. If the field of vision is changing too quickly, the saccades may never be able to arrive at and focus on a target, instead, the objects in view are only perceived through peripheral vision.</description></item><item><title>Serge Modular Archive Instrument</title><link>https://tedmoore.github.io/research/serge/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/research/serge/</guid><description>check out the GitHub Repo for this project.
The Serge Modular Archive Instrument (created in collaboration with Jean Brazeau) is a sample-based computer emulation of selected patches on the vintage Serge Modular instrument that is housed at Simon Frasier University in Vancouver, Canada. The project is conceived of as both an instrument for sonic exploration and an archive of the sound worlds made by this 50+ year old instrument, including (or highlighting) all of the idiosyncrasies it has accumulated over the years.</description></item><item><title>shadow</title><link>https://tedmoore.github.io/works/shadow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/shadow/</guid><description>shadow exists as both a solo version for no-input mixer and lights (performed at Omaha Under the Radar in 2018) and as a duo with saxophone. In this video, shadow was played as one movement of a longer performance on the Frequency Series at Constellation in Chicago.
The sounds of the no-input mixer are analyzed in real-time using timbral descriptors, which are then sent to a neural network for classification into one of four categories: distorted noise, high squeal, low impulses, or quiet sustained noise.</description></item></channel></rss>