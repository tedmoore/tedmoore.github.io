<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>video on Ted Moore | Composer, Improviser, Intermedia Artist</title><link>https://tedmoore.github.io/tags/video/</link><description>Recent content in video on Ted Moore | Composer, Improviser, Intermedia Artist</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Ted Moore. All Rights Reserved.</copyright><lastBuildDate>Wed, 01 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://tedmoore.github.io/tags/video/index.xml" rel="self" type="application/rss+xml"/><item><title>Making Polygons Morph and Float</title><link>https://tedmoore.github.io/blog/moprhing-polygons/</link><pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/moprhing-polygons/</guid><description>Finding a Polygon&amp;rsquo;s Centroid The polygon to find the centroid of. Take each vertex... ...get the x positions and average them to get the centroid&amp;#39;s x position. Do the same with the y positions to get the centroid&amp;#39;s y position. Use that x, y coordinate for the polygon&amp;#39;s centroid. ❮ ❯ 1 / 5 .</description></item><item><title>Learned Navigation of StyleGAN3 Latent Space from Audio Descriptors</title><link>https://tedmoore.github.io/blog/learned-navigation-stylegan3/</link><pubDate>Thu, 25 Sep 2025 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/learned-navigation-stylegan3/</guid><description>In September 2025, I presented a paper at the 6th Artificial Intelligence and Music Creativity Conference, which was held in Brussels Belgium. My paper described a process creating an audio reactive video (for the fourth movement of arco) using some machine learning in an attempt to intimately link the audio and video morphologies.
You can read the full paper here: Learned Navigation of StyleGAN3 Latent Space from Audio Descriptors
Screenshot of SuperCollider program developed for creating the audio reactive video.</description></item><item><title>Adapting _saccades_ for the Allosphere</title><link>https://tedmoore.github.io/blog/allosphere/</link><pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/allosphere/</guid><description>published May 15, 2025
In spring of 2024 I was asked to present saccades in the Allosphere: a three-story, 360 degree video sphere with 54 channel surround sound at UC Santa Barbara. The audience watches from a bridge that goes directly through the middle of the sphere. My saxophone-playing collaborator Kyle Hutchins was doing a west coast tour and though it would be cool to bring saccades to this very tech-forward space.</description></item><item><title>Creating *quartet*</title><link>https://tedmoore.github.io/blog/creating-quartet/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/blog/creating-quartet/</guid><description>published May 15, 2025
I created quartet in collaboration with the [Switch Ensemble~] for the 2021 SEAMUS Conference which was held online because of the pandemic. Because I knew that the work&amp;rsquo;s &amp;ldquo;premiere&amp;rdquo; would be held as a prerecorded video concert on YouTube, I decided to approach the work not as a live, electro-acoustic composition, but as a fixed-media, video-art piece that made use of the of the necessary technological mediation at play.</description></item><item><title>APPETITE</title><link>https://tedmoore.github.io/works/appetite/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/appetite/</guid><description>Created in collaboration with theater artist Charles Campbell of Skewed Visions.
​APPETITE uses the principle of feedback (audio, video, and cultural) to create environments that, while structured and intentional, are open, unpredictable, and greater than the elements used in their construction.
The texts in APPETITE use Chris Marker&amp;rsquo;s influential 1962 sci-fi/experimental film, La Jetée as a starting point. Marker&amp;rsquo;s time-travel love-story film is almost entirely made up of a sequence of still images with voice-over narration.</description></item><item><title>APSIS</title><link>https://tedmoore.github.io/works/apsis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/apsis/</guid><description>composed for and premiered by Strains New Music Ensemble
performance score
The video design was created mostly using Processing</description></item><item><title>arco</title><link>https://tedmoore.github.io/works/arco/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/arco/</guid><description>arco was motivated by an suggestion from violinist Marco Fusi to create a solo violin version of my work triangle for string quartet and tape. I liked the idea but felt that it would make more sense alongside additional movements to provide some context and commentary on this solo version. arco is a five movement work of which the solo version of triangle is movement four. I also created video designs to accompany the violinist and tape parts.</description></item><item><title>eclipse</title><link>https://tedmoore.github.io/works/eclipse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/eclipse/</guid><description> eclipse was commissioned by Skewed Visions, an experimental theater company in Minneapolis. The piece includes live audio processing using SuperCollider, as well as live visuals created with Processing. The visuals are controlled by the audio of the performer, giving the freedom to improvise within the structure specified by the composer, but keeping the visual and musical elements intimately connected through gesture.</description></item><item><title>fold</title><link>https://tedmoore.github.io/works/fold/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/fold/</guid><description>Read DigiScore’s analysis of fold
fold was commissioned to reflect the theme of &amp;ldquo;Location Sharing&amp;rdquo; for the 2023 Browser Sound Festival in Stuttgart. The ensemble in residence for the festival was a pianist who is in the room during the performance and a saxophonist and bass clarinetist who are remote (as in, in other countries). I chose to reflect the theme of &amp;ldquo;location sharing&amp;rdquo; by using audio feedback to &amp;ldquo;sound out&amp;rdquo; the resonant frequencies of the remote performers&amp;rsquo; locations.</description></item><item><title>for line upon line</title><link>https://tedmoore.github.io/works/for-line-upon-line/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/for-line-upon-line/</guid><description>My friends at line upon line percussion asked me to come up with an idea for a short piece that could be created collaboratively with them during the COVID time of physical distancing. In turn, I asked for videos of them making a collection of sounds that I could then combine into some groovy video thing.
The videos are played using custom software in c++ created with openframeworks, however all of the sequencing was done with patterns in SuperCollider.</description></item><item><title>nand</title><link>https://tedmoore.github.io/works/nand/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/nand/</guid><description>nand was premiered by the Splice Ensemble at their 2023 Splice Festival in Boston. nand is based on the timbre and rhythmic gating of the NAND-gate feedback circuit described by Nick Collins in Handmade Electronic Music. Even though the system is quite simple (producing repeating phrases consisting of square waves, filtered noise, and silence), each gesture has microvariations that increase the entropy and attract my attention endlessly. My favorite timbres from this circuit occur while parameters are being changed–when capacitors are firing at surprising times, before they can settle into a stasis.</description></item><item><title>noise/gate</title><link>https://tedmoore.github.io/works/noise-gate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/noise-gate/</guid><description> Composed for Giacomo Piermatti of Ensemble Suono Giallo for ilSuono Contemporary Music Week 2021. Città di Castello, Italy.
full score</description></item><item><title>quartet</title><link>https://tedmoore.github.io/works/quartet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/quartet/</guid><description>created with the [Switch~ Ensemble]:
Zach Sheets, flute
T.J. Borden, cello
Wei-Han Wu, piano
Megan Arns, percussion
quartet is a remote collaboration between myself and the [Switch~ Ensemble] designed to engage with the added technological mediation at play during the pandemic. The sonic source material of quartet is about two minutes of eurorack synthesizer recordings transcribed for the [Switch~ Ensemble] to record. These recordings were then subjected to data analysis using audio descriptors and machine learning algorithms using FluCoMa.</description></item><item><title>saccades</title><link>https://tedmoore.github.io/works/saccades/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/saccades/</guid><description>A “saccade” is a rapid movement of the eyeball between two fixed focal points. During this brief moment, the brain hides this blurry motion from our perception. Once a saccade motion has begun, the destination cannot change, meaning that if the target of focus disappears the viewer won’t know until the saccade completes. If the field of vision is changing too quickly, the saccades may never be able to arrive at and focus on a target, instead, the objects in view are only perceived through peripheral vision.</description></item><item><title>Solstice Orrery</title><link>https://tedmoore.github.io/works/solstice-orrery/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/solstice-orrery/</guid><description>Music composed by Scott Miller: Solstice Orrery on Raba (New Focus Recordings fcr198)
Video design by Ted Moore
Scott asked me to create a video design for this track (Solstice Orrery) on his New Focus release, Raba. An &amp;ldquo;orrery&amp;rdquo; is a mechanical model of the solar system (as can be heard in the music). I took inspiration from the title and used/created images of the sun in various renderings.</description></item><item><title>still motion</title><link>https://tedmoore.github.io/works/still-motion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/still-motion/</guid><description>Commissioned by and composed for percussionist Patti Cudd with support from MacPhail Center for Music and the McKnight Foundation.
still motion uses live audio and video sampling of the performing percussionist, the projection of which creates a counterpoint to the live performer. All of the sampling is done with an openFrameworks program coded in C++.
full score Contact me for the software download.</description></item><item><title>still motion b</title><link>https://tedmoore.github.io/works/still-motion-b/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/still-motion-b/</guid><description>still motion b uses live audio and video sampling of the performer&amp;rsquo;s mouth, the projection of which creates a counterpoint to the live performance. All of the sampling is done with an openFrameworks program coded in C++.
Contact me for the software download.</description></item><item><title>tap</title><link>https://tedmoore.github.io/works/tap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://tedmoore.github.io/works/tap/</guid><description>Commissioned by line upon line percussion
recorded at UT Austin, January 12, 2019
Composing tap began by recording many sounds, gestures, and passages on my eurorack modular synthesizer and then organizing these recordings into a tape part for the piece. I then transcribed the rhythms and timbres of the tape part to create a tight synchronization between the the tape and percussion parts played by the ensemble. The video and lights were then composed to add to the intense synchrony.</description></item></channel></rss>