---
title: research & software
draft: false
layout: single
---

Learn more [here](/research/flucoma) about my University of Huddersfield {{< el "CeReNeM" "https://research.hud.ac.uk/institutes-centres/cerenem/" >}} postdoc research on the [Fluid Corpus Manipulation](/research/flucoma) project.

# Selected Software

_Checkout my {{< el "github" "https://github.com/tedmoore" >}} for more._

* [DJII](/research/djii) modular live electronic fx commissioned by improvising bassoonist {{< el "Dana Jessen" "https://www.danajessen.com/" >}} (2022-) ({{< el "SuperCollider" "https://supercollider.github.io/" >}})
* [Mel-Frequency Cepstral Coefficients (MFCC) Interactive Explanation](https://learn.flucoma.org/reference/mfcc/explain/) (2022) (p5.js)
* [Serge Modular Archive Instrument](/research/serge) (2021-22) (SuperCollider, Processing, & C++ {{< el openFrameworks "https://openframeworks.cc/" >}})
* [Module-Tensor](/research/module-tensor): laptop improvisation software (2014-) (SuperCollider) (see [performances](/improv) of this software in use)
* [Microtonal Keyboard](/research/microtonal-keyboard) (2016) (SuperCollider)​

# Writing & Peer-Reviewed Research Publications

Moore, Ted, James Bradbury, PA Tremblay, and Owen Green. 2025. “Making Machine Learning Musical: Reflections on a Year of Teaching FluCoMa.” [*Journal SEAMUS 32*](https://seamusonline.org/wp-content/uploads/2025/01/Journal_SEAMUS_vol_32_rev_1.pdf), (1-2): 24-38.

Moore, Ted. 2024. ["Musical Agents, Agency, & AI: Towards a Phenomenological Understanding."](/documents/agency.pdf) In *Proceedings of the International Computer Music Conference (ICMC)*, Seoul, South Korea.

Moore, Ted and Jean Brazeau. 2023. "[Serge Modular Archive Instrument (SMAI): Bridging Skeuomorphic & Machine Learning Enabled Interfaces.](https://drive.google.com/file/d/1xwTn4mj6m55rGwtBfZMel1qxXs4GTbf0/view)" In *Proceedings of the New Interfaces for Musical Expression (NIME) Conference*, Mexico City, Mexico.

Moore, Ted. 2022. "[Expression, Collaboration, and Intuition.](https://archive.wetink.org/archive-06/expression-collaboration-and-intuition)" *Wet Ink Archive* 6.

Green, Owen, Pierre Alexandre Tremblay, Ted Moore, James Bradbury, Jacob Hart, Alex Harker, and Gerard Roma. 2022. "Architecture about Dancing: Creating a Cross Environment, Cross Domain Framework for Creative Coding Musicians." In [*Proceedings of the Psychology of Programming Interest Group (PPIG) Conference*](https://www.ppig.org/files/2022-PPIG-33rd--proceedings.pdf), 12-24. The Open University, Milton Keynes, UK.

Moore, Ted. 2021. "Polynomial Functions in Zuraj’s 'Changeover.'" [*Perspectives of New Music* 59 (1)](https://www.perspectivesofnewmusic.org/TOC/TOC591.pdf).

Moore, Ted. 2021. ["Human and Artificial Intelligence Alignment: AI as Musical Assistant and Collaborator."](/documents/phd-research-ted-moore.pdf) My PhD research (I'm calling ith this because technically my dissertation was a [composition](/works/tap).)

# Research Presentations

## Non-negative Matrix Factorization for Spatial Audio (2020)  
{{< columnrow >}}
{{% column 70 %}}
Due to COVID-19 the 2020 Spatial Music Workshop in the Cube at Virginia Tech was cancelled, but the organizers invited alumni to give talks about some aspect of their work with spatial audio. I presented my use of non-negative matrix factorization (NMF) for audio decomposition and spatialization. See the {{< el "NMF overview" "https://learn.flucoma.org/learn/bufnmf/" >}} I created for the FluCoMa project. 
{{< /column >}}
{{< column >}}
{{< figure src="/images/00_resynth_buffer_with_activations.jpg" width="400px" link="https://learn.flucoma.org/learn/bufnmf/" >}}
{{< /column >}}
{{< /columnrow >}}

## Interference Patterns: analysis of interacting feedbacks in _hollow_ (2020) {#interference-patterns}
{{< columnrow >}}
{{% column 70 %}}
This presentation analyzes the feedback system of my piece, [_hollow_](/works/hollow), which uses three large PVC tubes to create feedback at the resonant frequencies of the tubes. Through filtering, delay line modulation, and serial feedback routing, various emergent sonic properties arise. Analysis of the resulting sounds provides some insight into the behaviors of the system.

{{< el slides "/documents/chimefest-2020.pdf" >}}
{{< /column >}}
{{< column >}}
{{< figure src="/images/hollow.jpg" width="400px" link="/works/hollow" >}}
{{< /column >}}
{{< /columnrow >}}

## Preserving User-Defined Expression through Dimensionality Reduction (2019)
{{% columnrow %}}
{{% column 50 %}}
This is a talk a I gave at the FluCoMa Plenary Session at CeReNeM at the University of Huddersfield in the UK. It demonstrates various machine learning algorithms implemented in my improvisation software and how I use those algorithms to explore new modes of expressivity.  

{{< el slides "/documents/huddersfield_2019.pdf" >}}
{{% /column %}}
{{% column 50 %}}
{{% youtube jMZP_UF8gg0 %}}
{{% /column %}}
{{% /columnrow %}}

## ​Machine Learning Applications for Live Computer Music Performance (2019) {#dmw-talk}
{{% columnrow %}}
{{% column 50 %}}
Presentation at the University of Chicago Digital Media Workshop. This presentation demonstrates three uses of machine learning in live computer music performance: (1) using a {{< el "neural network" "https://github.com/tedmoore/NeuralNetwork" >}} to classify no-input mixer timbres for light control, (2) a frequency modulation synthesizer that predictions synthesis parameters based on novel incoming spectra, and (3) a {{< el "TSNE" "https://github.com/tedmoore/tSNE-SuperCollider" >}} based dimensionality reduction system for low-dimensional control of synthesizers with high-dimensional parameters spaces.

{{< el slides "/documents/dmw-slides.pdf" >}}
{{% /column %}}
{{% column 50 %}}
{{< figure src="/images/tsne-mapper-02.jpg" width="400px" >}}
{{% /column %}}
{{% /columnrow %}}

## Approaches to Live Performance and Composition with Machine Learning and Music Information Retrieval Analysis (2019)
{{% columnrow %}}
{{% column 50 %}}
This presentation offers three creative uses of machine learning: (1) using audio descriptor analysis and machine learning to organize grains of audio into a performable two dimensional space, (2) using a neural network to classify no-input mixer timbres for light control, and (3) using a traveling salesperson pathfinding algorithm to re-organize audio grains into a new sequence.  

{{< el slides "/documents/chimefest-2019.pdf" >}}
{{% /column %}}
{{% column 50 %}}
{{< figure src="/images/chimefest-2019.jpg" width="400px" >}}
{{% /column %}}
{{% /columnrow %}}