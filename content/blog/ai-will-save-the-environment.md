---
title: "\"AI will save the environment\""
description: ...a graph
draft: true
layout: single
featured_image: /images/binaryCanary.jpg
date: 2025-04-18
---

It's [well known](https://www.nytimes.com/2024/07/11/climate/artificial-intelligence-energy-usage.html) that training (and [deploying](https://www.nytimes.com/2025/04/24/technology/chatgpt-alexa-please-thank-you.html)) AI takes a lot of energy, a unfortunately hyping while we're trying to decrease energy usage to fight climate change. Microsoft is considering [purchasing](https://www.washingtonpost.com/business/2024/09/20/microsoft-three-mile-island-nuclear-constellation/
) it's own nuclear power facility to power it's AI systems (and emit less carbon...). 

There's a graph I've drawn on the board in a few classes. One axis is power consumption, the other axis is "intelligence" however one wants to define that, maybe "usefulness" of an AI system. Many AI companies want to declare their usefulness to be solving the problems of the world: medicine, education, climate change. In many cases, it is very true that there are [very real benefits](https://deepmind.google/technologies/alphafold/) that AI systems can offer, however usually from targeted AI systems learning from a dataset that is smaller (and more coherent) than the entirety of the internet. 

The assumption is that this is monotonic: that as power use goes up, intelligence will go up, in fact that *in order for* intelligence (e.g., profits) to go up power use *must* go up.

, there's a threshold on each axis, which one does the monotonic function reach first?